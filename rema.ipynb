{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "142e4a9f",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc8fe7f",
   "metadata": {},
   "source": [
    "## Analyse des Données de l'Article\n",
    "\n",
    "#### Deux Matrices Principales :\n",
    "\n",
    "| Matrice | Nb. Utilisateurs | Nb. Éléments (vidéos) | Nb. Interactions | Densité | Utilisation Principale |\n",
    "| :-- | :-- | :-- | :-- | :-- | :-- |\n",
    "| Petite matrice | 1 411 | 3 327 | 4 676 570 | 99,6% | Évaluation offline fiable |\n",
    "| Grande matrice | 7 176 | 10 728 | 12 530 806 | 16,3% | Entraînement du modèle |\n",
    "\n",
    "- Les interactions de la petite matrice sont exclues de la grande matrice pour garantir une séparation stricte entre les données d'entraînement et de test.\n",
    "\n",
    "---\n",
    "\n",
    "### Contenu des Fichiers et Métadonnées\n",
    "\n",
    "Le jeu de données comprend plusieurs fichiers CSV :\n",
    "\n",
    "- `big_matrix.csv` : Interactions utilisateur-élément (partiellement observées, pour l'entraînement)\n",
    "- `small_matrix.csv` : Interactions utilisateur-élément (presque entièrement observées, pour l'évaluation)\n",
    "- `user_features.csv` : 30 caractéristiques utilisateur (12 explicites, 18 encodées one-hot), incluant données démographiques, comportements, etc.\n",
    "- `item_daily_features.csv` : 56 caractéristiques d'éléments, dont 45 statistiques journalières (clics, j'aime, favoris, etc. pour chaque jour du 5 juillet au 5 septembre 2020)\n",
    "- `item_categories.csv` : Catégories/tags vidéo (31 tags, chaque vidéo possède 1 à 4 tags, ex. {Sports, Jeux})\n",
    "- `kuairec_caption_category.csv` : Légendes (descriptions textuelles) et catégories basées sur le texte pour chaque vidéo (ajouté en 2024 pour faciliter les modèles basés sur les LLM)\n",
    "- `social_network.csv` : Réseau d'amitié entre utilisateurs, utile pour les modèles sociaux ou hybrides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e912d7",
   "metadata": {},
   "source": [
    "## Extraction et Sélection des Caractéristiques pour le Modèle de Recommandation\n",
    "\n",
    "J'ai choisi d'implémenter un modèle basé sur le contenu des vidéos. \n",
    "\n",
    "Voici les données que j'ai identifiées comme les plus pertinentes :\n",
    "- *big_matrix* : Les **interactions utilisateur-vidéo** qui permettront de **calculer un score de préférence** pour chaque contenu visionné.\n",
    "- *item_categories* : Table associant les **vidéos** à leurs **catégories thématiques**. Ces données pourraient aider à **recommander des vidéos de thématiques similaires**. (Bien que considérée initialement, j'ai finalement privilégié d'autres caractéristiques plus significatives)\n",
    "- *kuairec_caption_category* : Contient les **classifications sémantiques** des vidéos. Les catégories de différents niveaux enrichiraient la **description des vidéos** pour mieux les faire correspondre aux **goûts de l'utilisateur**. (Également envisagée au départ mais non retenue au profit d'indicateurs plus forts)\n",
    "- *item_daily_features* : Regroupe les **métriques d'engagement quotidien** par vidéo, permettant d'identifier popularité, tendances et actualité du contenu. Ces données serviront à repérer les **vidéos tendance** correspondant aux préférences utilisateur (lectures, j'aimes, taux de visionnage complet). L'identifiant d'auteur facilitera la **recommandation de contenus du même créateur**.\n",
    "\n",
    "Dans l'approche basée sur le contenu adoptée, les tables social_network et user_features ne seront pas utilisées.\n",
    "\n",
    "Les descriptions textuelles complètes des vidéos, bien qu'informatives, nécessiteraient un traitement spécifique et ne seront donc pas intégrées au modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae896899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 10:55:23.444341: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747472123.462062 1584962 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747472123.467178 1584962 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747472123.481417 1584962 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747472123.481444 1584962 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747472123.481446 1584962 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747472123.481448 1584962 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-17 10:55:23.485632: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import h5py\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7927df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading big_matrix...\n",
      "Loading small_matrix...\n",
      "Creating sparse features...\n",
      "Aggregating metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10728/10728 [00:11<00:00, 895.93it/s]\n"
     ]
    }
   ],
   "source": [
    "dtypes = {\n",
    "    'user_id': 'int32',\n",
    "    'video_id': 'int32',\n",
    "    'play_duration': 'float32',\n",
    "    'video_duration': 'float32',\n",
    "    'watch_ratio': 'float32'\n",
    "}\n",
    "\n",
    "base_path = 'data_final_project/KuaiRec 2.0/data/'\n",
    "\n",
    "print(\"Loading big_matrix...\")\n",
    "big_matrix = pd.read_csv(f'{base_path}/big_matrix.csv', \n",
    "                       usecols=['user_id', 'video_id', 'watch_ratio'],\n",
    "                       dtype=dtypes)\n",
    "\n",
    "print(\"Loading small_matrix...\")\n",
    "small_matrix = pd.read_csv(f'{base_path}/small_matrix.csv', \n",
    "                         usecols=['user_id', 'video_id', 'watch_ratio'],\n",
    "                         dtype=dtypes)\n",
    "\n",
    "big_matrix['positive'] = (big_matrix['watch_ratio'] > 1.0).astype('int8')\n",
    "small_matrix['positive'] = (small_matrix['watch_ratio'] > 1.0).astype('int8')\n",
    "\n",
    "item_daily_features = pd.read_csv(f'{base_path}/item_daily_features.csv')\n",
    "\n",
    "print(\"Creating sparse features...\")\n",
    "video_features = item_daily_features.drop_duplicates(subset=['video_id'])[\n",
    "    ['video_id', 'author_id', 'video_duration']\n",
    "]\n",
    "\n",
    "engagement_columns = [\n",
    "    'show_cnt', 'play_cnt', 'play_duration', 'play_progress',\n",
    "    'complete_play_cnt', 'valid_play_cnt', 'long_time_play_cnt', \n",
    "    'like_cnt', 'comment_cnt', 'share_cnt'\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Aggregating metrics...\")\n",
    "agg_data = []\n",
    "for video_id in tqdm(video_features['video_id'].unique()):\n",
    "    video_rows = item_daily_features[item_daily_features['video_id'] == video_id]\n",
    "    if not video_rows.empty:\n",
    "        agg_row = video_rows[engagement_columns].mean().to_dict()\n",
    "        agg_row['video_id'] = video_id\n",
    "        agg_data.append(agg_row)\n",
    "\n",
    "agg_metrics = pd.DataFrame(agg_data)\n",
    "\n",
    "\n",
    "video_features = video_features.merge(agg_metrics, on='video_id', how='left')\n",
    "\n",
    "# Create derived features\n",
    "video_features['engagement_rate'] = (video_features['play_cnt'] / \n",
    "                                    (video_features['show_cnt'] + 1)).fillna(0)\n",
    "video_features['completion_rate'] = (video_features['complete_play_cnt'] / \n",
    "                                    (video_features['play_cnt'] + 1)).fillna(0)\n",
    "video_features['like_rate'] = (video_features['like_cnt'] / \n",
    "                              (video_features['play_cnt'] + 1)).fillna(0)\n",
    "video_features['avg_watch_ratio'] = (video_features['play_duration'] / \n",
    "                                   (video_features['video_duration'] + 1)).fillna(0)\n",
    "\n",
    "video_features = video_features.fillna(0)\n",
    "\n",
    "for col in video_features.columns:\n",
    "    if col not in ['video_id', 'author_id']:\n",
    "        video_features[col] = video_features[col].astype('float32')\n",
    "\n",
    "video_features.to_csv(f'{os.path.dirname(base_path)}/video_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30934ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions:\n",
      "Big Matrix: (12530806, 4)\n",
      "Small Matrix: (4676570, 4)\n",
      "Item Daily Features: (343341, 58)\n",
      "\n",
      "=== 1. GLOBAL DATA ANALYSIS ===\n",
      "\n",
      "Big matrix preview:\n",
      "   user_id  video_id  watch_ratio  positive\n",
      "0        0      3649     1.273396         1\n",
      "1        0      9598     1.244082         1\n",
      "2        0      5262     0.107613         0\n",
      "3        0      1963     0.089885         0\n",
      "4        0      8234     0.078000         0\n",
      "\n",
      "Small matrix preview:\n",
      "   user_id  video_id  watch_ratio  positive\n",
      "0       14       148     0.722103         0\n",
      "1       14       183     1.907377         1\n",
      "2       14      3649     2.063311         1\n",
      "3       14      5262     0.566388         0\n",
      "4       14      8234     0.418364         0\n",
      "\n",
      "Item daily features preview:\n",
      "   video_id      date  author_id video_type   upload_dt  upload_type  \\\n",
      "0         0  20200705       3309     NORMAL  2020-03-30  ShortImport   \n",
      "1         0  20200706       3309     NORMAL  2020-03-30  ShortImport   \n",
      "2         0  20200707       3309     NORMAL  2020-03-30  ShortImport   \n",
      "3         0  20200708       3309     NORMAL  2020-03-30  ShortImport   \n",
      "4         0  20200709       3309     NORMAL  2020-03-30  ShortImport   \n",
      "\n",
      "  visible_status  video_duration  video_width  video_height    music_id  \\\n",
      "0         public          5966.0          720          1280  3350323409   \n",
      "1         public          5966.0          720          1280  3350323409   \n",
      "2         public          5966.0          720          1280  3350323409   \n",
      "3         public          5966.0          720          1280  3350323409   \n",
      "4         public          5966.0          720          1280  3350323409   \n",
      "\n",
      "   video_tag_id video_tag_name  show_cnt  show_user_num  play_cnt  \\\n",
      "0           841             建筑     14665          11372     10141   \n",
      "1           841             建筑     10883           8513      7321   \n",
      "2           841             建筑      7842           6281      4757   \n",
      "3           841             建筑      8916           7229      5172   \n",
      "4           841             建筑      8502           6658      5392   \n",
      "\n",
      "   play_user_num  play_duration  complete_play_cnt  complete_play_user_num  \\\n",
      "0           7485       88729488               5657                    4834   \n",
      "1           5490       64264607               4162                    3522   \n",
      "2           3724       41338741               2734                    2403   \n",
      "3           3961       45281254               2950                    2525   \n",
      "4           3946       46952744               3058                    2566   \n",
      "\n",
      "   valid_play_cnt  valid_play_user_num  long_time_play_cnt  \\\n",
      "0            5503                 4775                5503   \n",
      "1            4039                 3468                4039   \n",
      "2            2640                 2376                2640   \n",
      "3            2865                 2498                2865   \n",
      "4            2946                 2533                2945   \n",
      "\n",
      "   long_time_play_user_num  short_time_play_cnt  short_time_play_user_num  \\\n",
      "0                     4775                 1939                      1481   \n",
      "1                     3468                 1340                      1040   \n",
      "2                     2376                  866                       683   \n",
      "3                     2498                  977                       765   \n",
      "4                     2532                 1046                       752   \n",
      "\n",
      "   play_progress  comment_stay_duration  like_cnt  like_user_num  \\\n",
      "0       0.799860                6629173       573            569   \n",
      "1       0.805253                3997498       302            301   \n",
      "2       0.808821                3314323       205            205   \n",
      "3       0.801680                4235579       297            293   \n",
      "4       0.805359                3862095       307            305   \n",
      "\n",
      "   click_like_cnt  double_click_cnt  cancel_like_cnt  cancel_like_user_num  \\\n",
      "0             315               257               87                    85   \n",
      "1             159               142               47                    47   \n",
      "2             121                84               52                    50   \n",
      "3             178               119               60                    59   \n",
      "4             166               141               57                    56   \n",
      "\n",
      "   comment_cnt  comment_user_num  direct_comment_cnt  reply_comment_cnt  \\\n",
      "0           11                11                   8                  3   \n",
      "1            7                 7                   6                  1   \n",
      "2            4                 3                   3                  1   \n",
      "3            4                 4                   2                  2   \n",
      "4            5                 2                   0                  5   \n",
      "\n",
      "   delete_comment_cnt  delete_comment_user_num  comment_like_cnt  \\\n",
      "0                   0                        0               112   \n",
      "1                   0                        0                60   \n",
      "2                   0                        0                59   \n",
      "3                   0                        0                91   \n",
      "4                   0                        0                76   \n",
      "\n",
      "   comment_like_user_num  follow_cnt  follow_user_num  cancel_follow_cnt  \\\n",
      "0                     61         284              284                  0   \n",
      "1                     32         201              200                  0   \n",
      "2                     26         131              131                  0   \n",
      "3                     46         179              179                  0   \n",
      "4                     47         186              186                  0   \n",
      "\n",
      "   cancel_follow_user_num  share_cnt  share_user_num  download_cnt  \\\n",
      "0                       0          2               2             8   \n",
      "1                       0          1               1             2   \n",
      "2                       0          1               1             2   \n",
      "3                       0          2               2             3   \n",
      "4                       0          0               0             2   \n",
      "\n",
      "   download_user_num  report_cnt  report_user_num  reduce_similar_cnt  \\\n",
      "0                  8           0                0                   3   \n",
      "1                  2           0                0                   5   \n",
      "2                  2           0                0                   0   \n",
      "3                  3           0                0                   3   \n",
      "4                  2           2                1                   1   \n",
      "\n",
      "   reduce_similar_user_num  collect_cnt  collect_user_num  cancel_collect_cnt  \\\n",
      "0                        3          NaN               NaN                 NaN   \n",
      "1                        5          NaN               NaN                 NaN   \n",
      "2                        0          NaN               NaN                 NaN   \n",
      "3                        3          NaN               NaN                 NaN   \n",
      "4                        1          NaN               NaN                 NaN   \n",
      "\n",
      "   cancel_collect_user_num  \n",
      "0                      NaN  \n",
      "1                      NaN  \n",
      "2                      NaN  \n",
      "3                      NaN  \n",
      "4                      NaN  \n",
      "\n",
      "Big matrix information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12530806 entries, 0 to 12530805\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   user_id      int32  \n",
      " 1   video_id     int32  \n",
      " 2   watch_ratio  float32\n",
      " 3   positive     int8   \n",
      "dtypes: float32(1), int32(2), int8(1)\n",
      "memory usage: 155.4 MB\n",
      "\n",
      "Item daily features information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 343341 entries, 0 to 343340\n",
      "Data columns (total 58 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   video_id                  343341 non-null  int64  \n",
      " 1   date                      343341 non-null  int64  \n",
      " 2   author_id                 343341 non-null  int64  \n",
      " 3   video_type                343341 non-null  object \n",
      " 4   upload_dt                 343341 non-null  object \n",
      " 5   upload_type               343341 non-null  object \n",
      " 6   visible_status            343341 non-null  object \n",
      " 7   video_duration            332743 non-null  float64\n",
      " 8   video_width               343341 non-null  int64  \n",
      " 9   video_height              343341 non-null  int64  \n",
      " 10  music_id                  343341 non-null  int64  \n",
      " 11  video_tag_id              343341 non-null  int64  \n",
      " 12  video_tag_name            310907 non-null  object \n",
      " 13  show_cnt                  343341 non-null  int64  \n",
      " 14  show_user_num             343341 non-null  int64  \n",
      " 15  play_cnt                  343341 non-null  int64  \n",
      " 16  play_user_num             343341 non-null  int64  \n",
      " 17  play_duration             343341 non-null  int64  \n",
      " 18  complete_play_cnt         343341 non-null  int64  \n",
      " 19  complete_play_user_num    343341 non-null  int64  \n",
      " 20  valid_play_cnt            343341 non-null  int64  \n",
      " 21  valid_play_user_num       343341 non-null  int64  \n",
      " 22  long_time_play_cnt        343341 non-null  int64  \n",
      " 23  long_time_play_user_num   343341 non-null  int64  \n",
      " 24  short_time_play_cnt       343341 non-null  int64  \n",
      " 25  short_time_play_user_num  343341 non-null  int64  \n",
      " 26  play_progress             343341 non-null  float64\n",
      " 27  comment_stay_duration     343341 non-null  int64  \n",
      " 28  like_cnt                  343341 non-null  int64  \n",
      " 29  like_user_num             343341 non-null  int64  \n",
      " 30  click_like_cnt            343341 non-null  int64  \n",
      " 31  double_click_cnt          343341 non-null  int64  \n",
      " 32  cancel_like_cnt           343341 non-null  int64  \n",
      " 33  cancel_like_user_num      343341 non-null  int64  \n",
      " 34  comment_cnt               343341 non-null  int64  \n",
      " 35  comment_user_num          343341 non-null  int64  \n",
      " 36  direct_comment_cnt        343341 non-null  int64  \n",
      " 37  reply_comment_cnt         343341 non-null  int64  \n",
      " 38  delete_comment_cnt        343341 non-null  int64  \n",
      " 39  delete_comment_user_num   343341 non-null  int64  \n",
      " 40  comment_like_cnt          343341 non-null  int64  \n",
      " 41  comment_like_user_num     343341 non-null  int64  \n",
      " 42  follow_cnt                343341 non-null  int64  \n",
      " 43  follow_user_num           343341 non-null  int64  \n",
      " 44  cancel_follow_cnt         343341 non-null  int64  \n",
      " 45  cancel_follow_user_num    343341 non-null  int64  \n",
      " 46  share_cnt                 343341 non-null  int64  \n",
      " 47  share_user_num            343341 non-null  int64  \n",
      " 48  download_cnt              343341 non-null  int64  \n",
      " 49  download_user_num         343341 non-null  int64  \n",
      " 50  report_cnt                343341 non-null  int64  \n",
      " 51  report_user_num           343341 non-null  int64  \n",
      " 52  reduce_similar_cnt        343341 non-null  int64  \n",
      " 53  reduce_similar_user_num   343341 non-null  int64  \n",
      " 54  collect_cnt               273658 non-null  float64\n",
      " 55  collect_user_num          273658 non-null  float64\n",
      " 56  cancel_collect_cnt        273658 non-null  float64\n",
      " 57  cancel_collect_user_num   273658 non-null  float64\n",
      "dtypes: float64(6), int64(47), object(5)\n",
      "memory usage: 151.9+ MB\n",
      "\n",
      "Big matrix descriptive statistics:\n",
      "           user_id     video_id  watch_ratio     positive\n",
      "count  12530806.00  12530806.00  12530806.00  12530806.00\n",
      "mean       3574.38      5057.75         0.94         0.34\n",
      "std        2067.01      3089.87         1.67         0.47\n",
      "min           0.00         0.00         0.00         0.00\n",
      "25%        1788.00      2387.00         0.31         0.00\n",
      "50%        3578.00      4822.00         0.72         0.00\n",
      "75%        5343.75      7600.00         1.18         1.00\n",
      "max        7175.00     10727.00       573.46         1.00\n",
      "\n",
      "Item daily features descriptive statistics:\n",
      "        video_id         date  author_id  video_duration  video_width  \\\n",
      "count  343341.00    343341.00  343341.00       332743.00    343341.00   \n",
      "mean     5077.37  20200803.15    4220.96        12508.37       713.12   \n",
      "std      3113.62        56.55    2390.32        13904.82       122.67   \n",
      "min         0.00  20200705.00       0.00           40.00       270.00   \n",
      "25%      2318.00  20200731.00    2126.00         6901.00       720.00   \n",
      "50%      5235.00  20200814.00    4402.00         9466.00       720.00   \n",
      "75%      7725.00  20200826.00    6293.00        12241.00       720.00   \n",
      "max     10727.00  20200905.00    8369.00       315040.00      3024.00   \n",
      "\n",
      "       video_height      music_id  video_tag_id     show_cnt  show_user_num  \\\n",
      "count     343341.00  3.433410e+05     343341.00    343341.00      343341.00   \n",
      "mean        1218.80  3.041715e+09       1363.17     94245.80       88599.75   \n",
      "std          165.72  1.543376e+09       1090.36    576170.04      551245.82   \n",
      "min          320.00  0.000000e+00       -124.00         0.00           0.00   \n",
      "25%         1280.00  2.354487e+09        144.00        55.00          47.00   \n",
      "50%         1280.00  3.921367e+09       1099.00      1328.00        1126.00   \n",
      "75%         1280.00  4.098158e+09       2491.00     24190.00       21616.00   \n",
      "max         3024.00  4.431044e+09       2891.00  42181538.00    38935328.00   \n",
      "\n",
      "          play_cnt  play_user_num  play_duration  complete_play_cnt  \\\n",
      "count    343341.00      343341.00   3.433410e+05          343341.00   \n",
      "mean      94187.81       84835.03   1.804975e+09           50753.86   \n",
      "std      593329.78      541359.07   1.605857e+10          356349.23   \n",
      "min           0.00           0.00   0.000000e+00               0.00   \n",
      "25%          14.00          12.00   1.017910e+05               4.00   \n",
      "50%         628.00         526.00   5.923139e+06             208.00   \n",
      "75%       20671.00       18014.00   2.486444e+08            8954.00   \n",
      "max    41167951.00    37222773.00   1.985890e+12        25234431.00   \n",
      "\n",
      "       complete_play_user_num  valid_play_cnt  valid_play_user_num  \\\n",
      "count                343341.0       343341.00            343341.00   \n",
      "mean                  48245.5        61956.22             59198.86   \n",
      "std                  344270.9       417720.69            404990.34   \n",
      "min                       0.0            0.00                 0.00   \n",
      "25%                       4.0            5.00                 5.00   \n",
      "50%                     186.0          280.00               253.00   \n",
      "75%                    8099.0        11373.00             10402.00   \n",
      "max                24299574.0     29185183.00          28227146.00   \n",
      "\n",
      "       long_time_play_cnt  long_time_play_user_num  short_time_play_cnt  \\\n",
      "count           343341.00                343341.00            343341.00   \n",
      "mean             52923.14                 50794.26             18751.47   \n",
      "std             367290.02                357991.33            115296.42   \n",
      "min                  0.00                     0.00                 0.00   \n",
      "25%                  4.00                     4.00                 6.00   \n",
      "50%                209.00                   190.00               182.00   \n",
      "75%               9127.00                  8315.00              4590.00   \n",
      "max           26497989.00              25808297.00          10428607.00   \n",
      "\n",
      "       short_time_play_user_num  play_progress  comment_stay_duration  \\\n",
      "count                 343341.00      343341.00           3.433410e+05   \n",
      "mean                   16885.51           0.38           9.853639e+07   \n",
      "std                   105420.91           0.28           1.199933e+09   \n",
      "min                        0.00           0.00           0.000000e+00   \n",
      "25%                        5.00           0.12           0.000000e+00   \n",
      "50%                      156.00           0.35           8.885900e+04   \n",
      "75%                     4010.00           0.64           3.921212e+06   \n",
      "max                  9142707.00           1.00           2.084713e+11   \n",
      "\n",
      "         like_cnt  like_user_num  click_like_cnt  double_click_cnt  \\\n",
      "count   343341.00      343341.00       343341.00         343341.00   \n",
      "mean      2781.11        2757.48         1371.50           1406.16   \n",
      "std      18696.03       18508.41         9628.63           9454.01   \n",
      "min          0.00           0.00            0.00              0.00   \n",
      "25%          0.00           0.00            0.00              0.00   \n",
      "50%          9.00           9.00            4.00              5.00   \n",
      "75%        421.00         417.00          192.00            211.00   \n",
      "max    2673037.00     2651624.00      1702560.00         965569.00   \n",
      "\n",
      "       cancel_like_cnt  cancel_like_user_num  comment_cnt  comment_user_num  \\\n",
      "count        343341.00             343341.00    343341.00         343341.00   \n",
      "mean            178.56                168.60       130.48            114.46   \n",
      "std            1557.68               1449.53      1565.61           1361.54   \n",
      "min               0.00                  0.00         0.00              0.00   \n",
      "25%               0.00                  0.00         0.00              0.00   \n",
      "50%               3.00                  2.00         0.00              0.00   \n",
      "75%              39.00                 37.00         9.00              8.00   \n",
      "max          233582.00             209037.00    182959.00         171602.00   \n",
      "\n",
      "       direct_comment_cnt  reply_comment_cnt  delete_comment_cnt  \\\n",
      "count           343341.00          343341.00           343341.00   \n",
      "mean                96.69              33.79                4.86   \n",
      "std               1225.04             506.86              134.20   \n",
      "min                  0.00               0.00                0.00   \n",
      "25%                  0.00               0.00                0.00   \n",
      "50%                  0.00               0.00                0.00   \n",
      "75%                  6.00               1.00                0.00   \n",
      "max             174878.00           99624.00            51898.00   \n",
      "\n",
      "       delete_comment_user_num  comment_like_cnt  comment_like_user_num  \\\n",
      "count                343341.00         343341.00              343341.00   \n",
      "mean                      4.08            749.79                 351.13   \n",
      "std                      87.22           7628.81                3379.50   \n",
      "min                       0.00              0.00                   0.00   \n",
      "25%                       0.00              0.00                   0.00   \n",
      "50%                       0.00              0.00                   0.00   \n",
      "75%                       0.00             20.00                  11.00   \n",
      "max                   18020.00         683828.00              307221.00   \n",
      "\n",
      "       follow_cnt  follow_user_num  cancel_follow_cnt  cancel_follow_user_num  \\\n",
      "count   343341.00        343341.00          343341.00               343341.00   \n",
      "mean       302.64           302.49               0.02                    0.02   \n",
      "std       2822.29          2820.67               0.28                    0.27   \n",
      "min          0.00             0.00               0.00                    0.00   \n",
      "25%          0.00             0.00               0.00                    0.00   \n",
      "50%          1.00             1.00               0.00                    0.00   \n",
      "75%         39.00            39.00               0.00                    0.00   \n",
      "max     493711.00        493128.00              30.00                   29.00   \n",
      "\n",
      "       share_cnt  share_user_num  download_cnt  download_user_num  report_cnt  \\\n",
      "count  343341.00       343341.00     343341.00           343341.0   343341.00   \n",
      "mean       80.38           74.79         44.56               39.7        0.04   \n",
      "std      1284.17         1186.87        535.58              471.3        0.82   \n",
      "min         0.00            0.00          0.00                0.0        0.00   \n",
      "25%         0.00            0.00          0.00                0.0        0.00   \n",
      "50%         0.00            0.00          0.00                0.0        0.00   \n",
      "75%         3.00            3.00          3.00                3.0        0.00   \n",
      "max    299286.00       279187.00     134099.00           133802.0      198.00   \n",
      "\n",
      "       report_user_num  reduce_similar_cnt  reduce_similar_user_num  \\\n",
      "count        343341.00           343341.00                343341.00   \n",
      "mean              0.04               50.42                    48.94   \n",
      "std               0.70              281.58                   273.24   \n",
      "min               0.00                0.00                     0.00   \n",
      "25%               0.00                0.00                     0.00   \n",
      "50%               0.00                0.00                     0.00   \n",
      "75%               0.00               10.00                    10.00   \n",
      "max             176.00            34454.00                 33720.00   \n",
      "\n",
      "       collect_cnt  collect_user_num  cancel_collect_cnt  \\\n",
      "count    273658.00         273658.00           273658.00   \n",
      "mean         20.01             19.84                1.32   \n",
      "std         356.68            353.74                7.58   \n",
      "min           0.00              0.00                0.00   \n",
      "25%           0.00              0.00                0.00   \n",
      "50%           0.00              0.00                0.00   \n",
      "75%           1.00              1.00                0.00   \n",
      "max      116971.00         115859.00             2056.00   \n",
      "\n",
      "       cancel_collect_user_num  \n",
      "count                273658.00  \n",
      "mean                      1.30  \n",
      "std                       7.36  \n",
      "min                       0.00  \n",
      "25%                       0.00  \n",
      "50%                       0.00  \n",
      "75%                       0.00  \n",
      "max                    1971.00  \n",
      "\n",
      "=== 2. DATA QUALITY ANALYSIS ===\n",
      "\n",
      "Missing values in big_matrix:\n",
      "Empty DataFrame\n",
      "Columns: [Missing Values, Percentage]\n",
      "Index: []\n",
      "\n",
      "Missing values in small_matrix:\n",
      "Empty DataFrame\n",
      "Columns: [Missing Values, Percentage]\n",
      "Index: []\n",
      "\n",
      "Missing values in item_daily_features:\n",
      "                         Missing Values  Percentage\n",
      "video_duration                    10598    3.086727\n",
      "video_tag_name                    32434    9.446585\n",
      "collect_cnt                       69683   20.295566\n",
      "collect_user_num                  69683   20.295566\n",
      "cancel_collect_cnt                69683   20.295566\n",
      "cancel_collect_user_num           69683   20.295566\n",
      "\n",
      "=== 3. USER ANALYSIS ===\n",
      "\n",
      "Statistics of interactions per user:\n",
      "count     7176.00\n",
      "mean      1746.21\n",
      "std        991.83\n",
      "min        100.00\n",
      "25%        883.00\n",
      "50%       1846.50\n",
      "75%       2461.00\n",
      "max      16015.00\n",
      "dtype: float64\n",
      "\n",
      "Statistics of average watch ratio per user:\n",
      "count    7176.00\n",
      "mean        0.96\n",
      "std         0.21\n",
      "min         0.13\n",
      "25%         0.81\n",
      "50%         0.93\n",
      "75%         1.07\n",
      "max         2.36\n",
      "Name: watch_ratio, dtype: float64\n",
      "\n",
      "=== 4. VIDEO ANALYSIS ===\n",
      "\n",
      "Statistics of interactions per video:\n",
      "count    10728.00\n",
      "mean      1168.05\n",
      "std       1599.93\n",
      "min          1.00\n",
      "25%         38.00\n",
      "50%        243.00\n",
      "75%       2142.50\n",
      "max      27615.00\n",
      "dtype: float64\n",
      "\n",
      "Top 10 most viewed videos:\n",
      "video_id\n",
      "8145     27615\n",
      "1037     12419\n",
      "9485     11134\n",
      "10174    10416\n",
      "7508      9986\n",
      "10552     9737\n",
      "4410      9509\n",
      "8716      8693\n",
      "1612      8322\n",
      "3293      8179\n",
      "dtype: int64\n",
      "\n",
      "Statistics of average watch ratio per video:\n",
      "count    10728.00\n",
      "mean         1.20\n",
      "std          2.15\n",
      "min          0.05\n",
      "25%          0.76\n",
      "50%          1.00\n",
      "75%          1.35\n",
      "max        135.66\n",
      "Name: watch_ratio, dtype: float64\n",
      "\n",
      "Top 10 videos with best watch ratio (min 10 interactions):\n",
      "video_id\n",
      "7445    70.601707\n",
      "6430     9.832020\n",
      "3849     7.706138\n",
      "7603     6.586339\n",
      "1132     5.786854\n",
      "6836     5.390290\n",
      "7857     5.090063\n",
      "9178     4.484676\n",
      "6554     4.150312\n",
      "1980     4.149167\n",
      "Name: watch_ratio, dtype: float32\n",
      "\n",
      "Top 10 videos with worst watch ratio (min 10 interactions):\n",
      "video_id\n",
      "4021    0.051598\n",
      "6268    0.055563\n",
      "408     0.055667\n",
      "1170    0.055853\n",
      "7196    0.056225\n",
      "1281    0.056253\n",
      "8708    0.056538\n",
      "7736    0.056551\n",
      "3978    0.056710\n",
      "2288    0.057617\n",
      "Name: watch_ratio, dtype: float32\n",
      "\n",
      "=== 5. WATCH RATIO DISTRIBUTION ANALYSIS ===\n",
      "\n",
      "Positive interactions (watch_ratio > 1.0): 4237441 (33.82%)\n",
      "Negative interactions (watch_ratio <= 1.0): 8293365 (66.18%)\n",
      "\n",
      "=== 6. VIDEO FEATURES ANALYSIS ===\n",
      "\n",
      "Descriptive statistics of engagement features:\n",
      "          show_cnt     play_cnt  play_duration  play_progress  \\\n",
      "count    343341.00    343341.00   3.433410e+05      343341.00   \n",
      "mean      94245.80     94187.81   1.804975e+09           0.38   \n",
      "std      576170.04    593329.78   1.605857e+10           0.28   \n",
      "min           0.00         0.00   0.000000e+00           0.00   \n",
      "25%          55.00        14.00   1.017910e+05           0.12   \n",
      "50%        1328.00       628.00   5.923139e+06           0.35   \n",
      "75%       24190.00     20671.00   2.486444e+08           0.64   \n",
      "max    42181538.00  41167951.00   1.985890e+12           1.00   \n",
      "\n",
      "       complete_play_cnt  valid_play_cnt  long_time_play_cnt    like_cnt  \\\n",
      "count          343341.00       343341.00           343341.00   343341.00   \n",
      "mean            50753.86        61956.22            52923.14     2781.11   \n",
      "std            356349.23       417720.69           367290.02    18696.03   \n",
      "min                 0.00            0.00                0.00        0.00   \n",
      "25%                 4.00            5.00                4.00        0.00   \n",
      "50%               208.00          280.00              209.00        9.00   \n",
      "75%              8954.00        11373.00             9127.00      421.00   \n",
      "max          25234431.00     29185183.00         26497989.00  2673037.00   \n",
      "\n",
      "       comment_cnt  share_cnt  \n",
      "count    343341.00  343341.00  \n",
      "mean        130.48      80.38  \n",
      "std        1565.61    1284.17  \n",
      "min           0.00       0.00  \n",
      "25%           0.00       0.00  \n",
      "50%           0.00       0.00  \n",
      "75%           9.00       3.00  \n",
      "max      182959.00  299286.00  \n",
      "\n",
      "=== 7. ANALYSIS OF RELATIONSHIPS BETWEEN FEATURES AND WATCH RATIO ===\n",
      "\n",
      "Correlation of features with watch ratio:\n",
      "watch_ratio           1.000000\n",
      "completion_rate       0.069843\n",
      "video_id              0.011920\n",
      "like_rate             0.008364\n",
      "author_id            -0.004586\n",
      "comment_cnt          -0.021039\n",
      "avg_watch_ratio      -0.026401\n",
      "share_cnt            -0.031192\n",
      "complete_play_cnt    -0.031406\n",
      "long_time_play_cnt   -0.036217\n",
      "valid_play_cnt       -0.039978\n",
      "play_cnt             -0.043308\n",
      "show_cnt             -0.044092\n",
      "like_cnt             -0.044880\n",
      "play_duration        -0.045319\n",
      "play_progress        -0.051596\n",
      "video_duration       -0.110227\n",
      "engagement_rate      -0.139784\n",
      "Name: watch_ratio, dtype: float64\n",
      "\n",
      "=== 8. SEGMENTATION AND ADVANCED ANALYSIS ===\n",
      "\n",
      "=== 9. RECOMMENDATION THRESHOLD ANALYSIS ===\n",
      "\n",
      "=== 10. SUMMARY AND INSIGHTS ===\n",
      "Number of users: 7176\n",
      "Number of videos: 10728\n",
      "Total number of interactions: 12530806\n",
      "Average interactions per user: 1746.21\n",
      "Matrix density: 0.162771 (16.2771%)\n",
      "Average watch ratio: 0.9445\n",
      "Median watch ratio: 0.7235\n",
      "Positive interactions rate (>1.0): 33.82%\n",
      "\n",
      "Users with highest watch_ratio variance (potentially difficult to predict):\n",
      "user_id\n",
      "498     398.207550\n",
      "4038    338.347626\n",
      "5590    171.054962\n",
      "4632    149.687103\n",
      "6337    134.511017\n",
      "5945    115.558151\n",
      "3339    102.988274\n",
      "5689     93.221413\n",
      "2578     85.915367\n",
      "5924     77.602440\n",
      "Name: watch_ratio, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "base_path_pics = 'data_final_project/data_analysis/'\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('whitegrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Data dimensions:\")\n",
    "print(f\"Big Matrix: {big_matrix.shape}\")\n",
    "print(f\"Small Matrix: {small_matrix.shape}\")\n",
    "print(f\"Item Daily Features: {item_daily_features.shape}\")\n",
    "\n",
    "print(\"\\n=== 1. GLOBAL DATA ANALYSIS ===\")\n",
    "\n",
    "print(\"\\nBig matrix preview:\")\n",
    "print(big_matrix.head())\n",
    "\n",
    "print(\"\\nSmall matrix preview:\")\n",
    "print(small_matrix.head())\n",
    "\n",
    "print(\"\\nItem daily features preview:\")\n",
    "print(item_daily_features.head())\n",
    "\n",
    "print(\"\\nBig matrix information:\")\n",
    "big_matrix.info()\n",
    "\n",
    "print(\"\\nItem daily features information:\")\n",
    "item_daily_features.info()\n",
    "\n",
    "print(\"\\nBig matrix descriptive statistics:\")\n",
    "print(big_matrix.describe().round(2))\n",
    "\n",
    "print(\"\\nItem daily features descriptive statistics:\")\n",
    "print(item_daily_features.describe().round(2))\n",
    "\n",
    "print(\"\\n=== 2. DATA QUALITY ANALYSIS ===\")\n",
    "\n",
    "def check_missing_values(df, name=\"DataFrame\"):\n",
    "    missing = df.isnull().sum()\n",
    "    missing_percent = (missing / len(df)) * 100\n",
    "    missing_data = pd.DataFrame({'Missing Values': missing, \n",
    "                                 'Percentage': missing_percent})\n",
    "    print(f\"\\nMissing values in {name}:\")\n",
    "    print(missing_data[missing_data['Missing Values'] > 0])\n",
    "    return missing_data[missing_data['Missing Values'] > 0].shape[0] > 0\n",
    "\n",
    "has_missing_big = check_missing_values(big_matrix, \"big_matrix\")\n",
    "has_missing_small = check_missing_values(small_matrix, \"small_matrix\")\n",
    "has_missing_features = check_missing_values(item_daily_features, \"item_daily_features\")\n",
    "\n",
    "print(\"\\n=== 3. USER ANALYSIS ===\")\n",
    "\n",
    "user_interaction_counts = big_matrix.groupby('user_id').size()\n",
    "print(\"\\nStatistics of interactions per user:\")\n",
    "print(user_interaction_counts.describe().round(2))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(user_interaction_counts, kde=True)\n",
    "plt.title(\"Distribution of interactions per user\")\n",
    "plt.xlabel(\"Number of interactions\")\n",
    "plt.ylabel(\"Number of users\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(base_path_pics + \"user_interaction_counts.png\")\n",
    "plt.close()\n",
    "\n",
    "avg_watch_ratio_by_user = big_matrix.groupby('user_id')['watch_ratio'].mean()\n",
    "print(\"\\nStatistics of average watch ratio per user:\")\n",
    "print(avg_watch_ratio_by_user.describe().round(2))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(avg_watch_ratio_by_user, kde=True)\n",
    "plt.title(\"Distribution of average watch ratio per user\")\n",
    "plt.xlabel(\"Average watch ratio\")\n",
    "plt.ylabel(\"Number of users\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(base_path_pics + \"avg_watch_ratio_by_user.png\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "print(\"\\n=== 4. VIDEO ANALYSIS ===\")\n",
    "\n",
    "video_interaction_counts = big_matrix.groupby('video_id').size()\n",
    "print(\"\\nStatistics of interactions per video:\")\n",
    "print(video_interaction_counts.describe().round(2))\n",
    "\n",
    "top_videos = video_interaction_counts.sort_values(ascending=False).head(10)\n",
    "print(\"\\nTop 10 most viewed videos:\")\n",
    "print(top_videos)\n",
    "\n",
    "avg_watch_ratio_by_video = big_matrix.groupby('video_id')['watch_ratio'].mean()\n",
    "print(\"\\nStatistics of average watch ratio per video:\")\n",
    "print(avg_watch_ratio_by_video.describe().round(2))\n",
    "\n",
    "top_watched_videos = avg_watch_ratio_by_video[video_interaction_counts > 10].sort_values(ascending=False).head(10)\n",
    "print(\"\\nTop 10 videos with best watch ratio (min 10 interactions):\")\n",
    "print(top_watched_videos)\n",
    "\n",
    "least_watched_videos = avg_watch_ratio_by_video[video_interaction_counts > 10].sort_values().head(10)\n",
    "print(\"\\nTop 10 videos with worst watch ratio (min 10 interactions):\")\n",
    "print(least_watched_videos)\n",
    "\n",
    "print(\"\\n=== 5. WATCH RATIO DISTRIBUTION ANALYSIS ===\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(big_matrix['watch_ratio'], bins=50, kde=True)\n",
    "plt.axvline(x=1.0, color='red', linestyle='--', label='Positive threshold (1.0)')\n",
    "plt.title(\"Distribution of watch ratios\")\n",
    "plt.xlabel(\"Watch ratio\")\n",
    "plt.ylabel(\"Number of interactions\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(base_path_pics +\"watch_ratio_distribution.png\")\n",
    "plt.close()\n",
    "\n",
    "positive_count = big_matrix['positive'].sum()\n",
    "negative_count = len(big_matrix) - positive_count\n",
    "print(f\"\\nPositive interactions (watch_ratio > 1.0): {positive_count} ({positive_count/len(big_matrix)*100:.2f}%)\")\n",
    "print(f\"Negative interactions (watch_ratio <= 1.0): {negative_count} ({negative_count/len(big_matrix)*100:.2f}%)\")\n",
    "\n",
    "\n",
    "print(\"\\n=== 6. VIDEO FEATURES ANALYSIS ===\")\n",
    "\n",
    "engagement_columns = [\n",
    "    'show_cnt', 'play_cnt', 'play_duration', 'play_progress',\n",
    "    'complete_play_cnt', 'valid_play_cnt', 'long_time_play_cnt', \n",
    "    'like_cnt', 'comment_cnt', 'share_cnt'\n",
    "]\n",
    "\n",
    "print(\"\\nDescriptive statistics of engagement features:\")\n",
    "print(item_daily_features[engagement_columns].describe().round(2))\n",
    "\n",
    "corr_matrix = video_features[['engagement_rate', 'completion_rate', 'like_rate', \n",
    "                             'avg_watch_ratio', 'video_duration'] + engagement_columns].corr()\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title(\"Correlation matrix of video features\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(base_path_pics +\"feature_correlation_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "print(\"\\n=== 7. ANALYSIS OF RELATIONSHIPS BETWEEN FEATURES AND WATCH RATIO ===\")\n",
    "\n",
    "\n",
    "video_avg_watch = big_matrix.groupby('video_id')['watch_ratio'].mean().reset_index()\n",
    "video_analysis = pd.merge(video_features, video_avg_watch, on='video_id', how='inner')\n",
    "\n",
    "\n",
    "watch_ratio_corr = video_analysis.corr()['watch_ratio'].sort_values(ascending=False)\n",
    "print(\"\\nCorrelation of features with watch ratio:\")\n",
    "print(watch_ratio_corr)\n",
    "\n",
    "top_features = watch_ratio_corr[1:6].index.tolist()\n",
    "for feature in top_features:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(video_analysis[feature], video_analysis['watch_ratio'], alpha=0.5)\n",
    "    plt.title(f\"Relationship between {feature} and watch ratio\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Average watch ratio\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(base_path_pics + f\"{feature}_vs_watch_ratio.png\")\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\n=== 8. SEGMENTATION AND ADVANCED ANALYSIS ===\")\n",
    "\n",
    "feature_cols = ['engagement_rate', 'completion_rate', 'like_rate', \n",
    "                'avg_watch_ratio', 'video_duration']\n",
    "X = video_analysis[feature_cols].copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "pca_df = pd.DataFrame({\n",
    "    'PC1': X_pca[:, 0],\n",
    "    'PC2': X_pca[:, 1],\n",
    "    'video_id': video_analysis['video_id'],\n",
    "    'watch_ratio': video_analysis['watch_ratio']\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(pca_df['PC1'], pca_df['PC2'], c=pca_df['watch_ratio'], \n",
    "                     cmap='viridis', alpha=0.6, s=50)\n",
    "plt.colorbar(scatter, label='Average watch ratio')\n",
    "plt.title(\"PCA projection of videos colored by watch ratio\")\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.2%} explained variance)\")\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.2%} explained variance)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(base_path_pics +\"pca_videos_by_watch_ratio.png\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "print(\"\\n=== 9. RECOMMENDATION THRESHOLD ANALYSIS ===\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "thresholds = [0.5, 0.8, 1.0, 1.2, 1.5, 2.0]\n",
    "for threshold in thresholds:\n",
    "    positive_ratio = (big_matrix['watch_ratio'] > threshold).mean()\n",
    "    plt.axvline(x=threshold, linestyle='--', label=f'Threshold {threshold} ({positive_ratio:.2%})')\n",
    "    \n",
    "sns.histplot(big_matrix['watch_ratio'], bins=100, kde=True)\n",
    "plt.title(\"Watch ratio distribution with different thresholds\")\n",
    "plt.xlabel(\"Watch ratio\")\n",
    "plt.ylabel(\"Number of interactions\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(base_path_pics +\"watch_ratio_thresholds.png\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "print(\"\\n=== 10. SUMMARY AND INSIGHTS ===\")\n",
    "\n",
    "n_users = big_matrix['user_id'].nunique()\n",
    "n_videos = big_matrix['video_id'].nunique()\n",
    "n_interactions = len(big_matrix)\n",
    "avg_interactions_per_user = n_interactions / n_users\n",
    "density = n_interactions / (n_users * n_videos)\n",
    "avg_watch_ratio = big_matrix['watch_ratio'].mean()\n",
    "median_watch_ratio = big_matrix['watch_ratio'].median()\n",
    "positive_rate = big_matrix['positive'].mean()\n",
    "\n",
    "print(f\"Number of users: {n_users}\")\n",
    "print(f\"Number of videos: {n_videos}\")\n",
    "print(f\"Total number of interactions: {n_interactions}\")\n",
    "print(f\"Average interactions per user: {avg_interactions_per_user:.2f}\")\n",
    "print(f\"Matrix density: {density:.6f} ({density*100:.4f}%)\")\n",
    "print(f\"Average watch ratio: {avg_watch_ratio:.4f}\")\n",
    "print(f\"Median watch ratio: {median_watch_ratio:.4f}\")\n",
    "print(f\"Positive interactions rate (>1.0): {positive_rate:.2%}\")\n",
    "\n",
    "user_variance = big_matrix.groupby('user_id')['watch_ratio'].var().sort_values(ascending=False)\n",
    "high_var_users = user_variance.head(10)\n",
    "\n",
    "print(\"\\nUsers with highest watch_ratio variance (potentially difficult to predict):\")\n",
    "print(high_var_users)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2771bbf",
   "metadata": {},
   "source": [
    "# Analyse des Données du Jeu de Données\n",
    "\n",
    "## Aperçu Général du Jeu de Données\n",
    "\n",
    "En examinant le jeu de données, j'ai découvert une densité étonnamment élevée (16,28%) pour une matrice d'interactions ! C'est très positif et suggère que les utilisateurs revisionnent certaines parties des vidéos. Cela justifie l'utilisation d'une régression plutôt qu'une classification, comme je l'avais initialement envisagé.\n",
    "\n",
    "## Analyse des Utilisateurs\n",
    "\n",
    "En approfondissant les statistiques utilisateurs, j'ai remarqué d'énormes variations dans la manière dont les personnes interagissent avec le contenu. Certains utilisateurs n'ont que 100 interactions tandis que d'autres en comptent plus de 16 000. L'écart-type est d'environ 991, ce qui indique clairement différents types d'utilisateurs.\n",
    "\n",
    "Plus intéressant encore, l'étendue des ratios moyens de visionnage par utilisateur varie de 0,13 à 2,36. Certains regardent à peine les vidéos tandis que d'autres révisionnent régulièrement le contenu. Mon modèle doit capturer ces différences, je vais donc absolument conserver les embeddings utilisateurs dans mon architecture.\n",
    "\n",
    "## Analyse des Vidéos\n",
    "\n",
    "Les statistiques vidéo étaient encore plus révélatrices. Certaines vidéos n'ont qu'une seule interaction tandis que d'autres en comptent 27 615. Les ratios de visionnage varient de 0,05 à 135,66. Ce dernier chiffre semble aberrant mais après vérification, ce n'est pas une erreur. Il doit s'agir d'une vidéo où les gens revisionnent constamment certaines parties ou laissent simplement leur téléphone allumé pendant qu'ils s'absentent.\n",
    "\n",
    "J'ai tracé quelques corrélations pour déterminer les caractéristiques les plus importantes. Le taux d'achèvement (completion_rate) présente la corrélation positive la plus forte avec le ratio de visionnage (0,07) - pas très forte mais certainement utile à inclure. Étonnamment, le taux d'engagement a une corrélation négative (-0,14). Je m'attendais franchement au contraire. Même constat pour la durée de la vidéo (-0,11), ce qui est logique - les gens regardent probablement les vidéos courtes plus intégralement.\n",
    "\n",
    "## Problèmes de Qualité des Données\n",
    "\n",
    "J'ai rencontré quelques problèmes de données manquantes. Environ 20% des métriques liées aux collections sont absentes, et près de 10% des vidéos n'ont pas d'étiquettes. C'est pourquoi j'ai décidé de ne pas utiliser ces caractéristiques - inutile de gérer autant de données manquantes quand j'ai d'autres caractéristiques complètes.\n",
    "\n",
    "La durée des vidéos est manquante pour 3,1% des vidéos, ce qui n'est pas catastrophique. J'ai décidé d'imputer ces valeurs avec la durée médiane puisque cette caractéristique est nécessaire (elle présente une corrélation significative avec le ratio de visionnage).\n",
    "\n",
    "## Seuils et Distribution\n",
    "\n",
    "Durant mon analyse, j'ai passé du temps à déterminer quel seuil était pertinent pour les interactions \"positives\". Il était utile de constater que 33,82% des interactions ont un ratio de visionnage > 1,0, et que le ratio médian de visionnage par vidéo est exactement 1,0. Cela m'a convaincu que mon choix de seuil était raisonnable.\n",
    "\n",
    "## Décisions d'Ingénierie des Caractéristiques\n",
    "\n",
    "En me basant sur mes découvertes de corrélation, j'ai créé quelques caractéristiques composites :\n",
    "1. Taux d'achèvement - qui montre la meilleure corrélation avec le ratio de visionnage\n",
    "2. Taux d'engagement - même s'il présente une corrélation négative, c'est un signal fort\n",
    "3. Ratio durée de visionnage - pour normaliser les différentes longueurs de vidéo\n",
    "\n",
    "J'ai été surpris que le taux de j'aime soit à peine corrélé avec le ratio de visionnage (0,008). Je pensais que les gens regarderaient plus attentivement le contenu qu'ils apprécient, mais les données ne confirment pas cette hypothèse. C'est pourquoi je me suis davantage concentré sur les métriques de comportement de visionnage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_train_prepare(big_matrix, video_features, batch_size=100000):\n",
    "    \"\"\"Prepare training data in batches to avoid memory issues\"\"\"\n",
    "    users = big_matrix['user_id'].unique()\n",
    "    \n",
    "    feature_cols = [col for col in video_features.columns \n",
    "                   if col not in ['video_id', 'author_id']]\n",
    "    \n",
    "    print(\"Fitting scaler...\")\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(video_features[feature_cols].values)\n",
    "    \n",
    "    print(\"Creating feature lookup dictionary...\")\n",
    "    video_feat_dict = {}\n",
    "    for _, row in tqdm(video_features.iterrows(), total=len(video_features)):\n",
    "        video_id = row['video_id']\n",
    "        feats = row[feature_cols].values.astype('float32')\n",
    "        video_feat_dict[video_id] = scaler.transform([feats])[0]\n",
    "    \n",
    "    return users, feature_cols, scaler, video_feat_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b2d92029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting scaler...\n",
      "Creating feature lookup dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10728/10728 [00:03<00:00, 2977.47it/s]\n"
     ]
    }
   ],
   "source": [
    "users, feature_cols, scaler, video_feat_dict = batch_train_prepare(big_matrix, video_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b39328",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = big_matrix['user_id'].max() + 1\n",
    "n_videos = max(big_matrix['video_id'].max(), small_matrix['video_id'].max()) + 1\n",
    "n_features = len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817308c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_memory_efficient_model(n_users, n_videos, n_features, embedding_dim=32):\n",
    "    \"\"\"Build a memory-efficient recommendation model\"\"\"\n",
    "    # Input layers\n",
    "    user_input = layers.Input(shape=(1,), name='user_id', dtype='int32')\n",
    "    video_input = layers.Input(shape=(1,), name='video_id', dtype='int32')\n",
    "    features_input = layers.Input(shape=(n_features,), name='video_features', dtype='float32')\n",
    "    \n",
    "    # User embedding\n",
    "    user_embedding = layers.Embedding(\n",
    "        n_users, embedding_dim, \n",
    "        embeddings_initializer='he_normal',\n",
    "        name='user_embedding'\n",
    "    )(user_input)\n",
    "    user_embedding = layers.Flatten()(user_embedding)\n",
    "    \n",
    "    # Video embedding\n",
    "    video_embedding = layers.Embedding(\n",
    "        n_videos, embedding_dim,\n",
    "        embeddings_initializer='he_normal',\n",
    "        name='video_embedding'\n",
    "    )(video_input)\n",
    "    video_embedding = layers.Flatten()(video_embedding)\n",
    "    \n",
    "    # Simpler feature processing\n",
    "    features_dense = layers.Dense(embedding_dim, activation='relu')(features_input)\n",
    "    \n",
    "    # Combine embeddings\n",
    "    concat_embeddings = layers.Concatenate()(\n",
    "        [user_embedding, video_embedding, features_dense]\n",
    "    )\n",
    "    \n",
    "    # network\n",
    "    x = layers.Dense(32, activation='relu')(concat_embeddings)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    \n",
    "    # Output layer\n",
    "    output = layers.Dense(1, activation=None)(x)  # Linear activation for regression\n",
    "    \n",
    "    model = models.Model(\n",
    "        inputs=[user_input, video_input, features_input],\n",
    "        outputs=output\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ba0cf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with dimensions: Users=7176, Videos=10728, Features=15\n"
     ]
    }
   ],
   "source": [
    "print(f\"Building model with dimensions: Users={n_users}, Videos={n_videos}, Features={n_features}\")\n",
    "model = build_memory_efficient_model(n_users, n_videos, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b91a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_data(data_chunk, video_feat_dict, feature_cols):\n",
    "    \"\"\"Process a batch of interaction data\"\"\"\n",
    "    user_ids = []\n",
    "    video_ids = []\n",
    "    features = []\n",
    "    watch_ratios = []\n",
    "    \n",
    "    for _, row in data_chunk.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        video_id = row['video_id']\n",
    "        ratio = row['watch_ratio']\n",
    "        \n",
    "        if video_id in video_feat_dict:\n",
    "            user_ids.append(user_id)\n",
    "            video_ids.append(video_id)\n",
    "            features.append(video_feat_dict[video_id])\n",
    "            watch_ratios.append(ratio)\n",
    "    \n",
    "    if not user_ids:\n",
    "        return None, None\n",
    "    \n",
    "    return {\n",
    "        'user_id': np.array(user_ids, dtype=np.int32),\n",
    "        'video_id': np.array(video_ids, dtype=np.int32),\n",
    "        'video_features': np.array(features, dtype=np.float32)\n",
    "    }, np.array(watch_ratios, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2887fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_in_batches(model, big_matrix, video_feat_dict, feature_cols, \n",
    "                          batch_size=50000, epochs=1, validation_split=0.1):\n",
    "    \"\"\"Train the model using batch processing to save memory\"\"\"\n",
    "    print(f\"Training model in batches of {batch_size}...\")\n",
    "    \n",
    "    big_matrix = big_matrix.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    val_size = int(len(big_matrix) * validation_split)\n",
    "    train_matrix = big_matrix[val_size:]\n",
    "    val_matrix = big_matrix[:val_size]\n",
    "    \n",
    "    val_inputs, val_labels = batch_process_data(val_matrix, video_feat_dict, feature_cols)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        total_batches = len(train_matrix) // batch_size + (1 if len(train_matrix) % batch_size > 0 else 0)\n",
    "        \n",
    "        for i in tqdm(range(total_batches)):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, len(train_matrix))\n",
    "            \n",
    "            batch_df = train_matrix.iloc[start_idx:end_idx]\n",
    "            batch_inputs, batch_labels = batch_process_data(batch_df, video_feat_dict, feature_cols)\n",
    "            \n",
    "            if batch_inputs is not None:\n",
    "                model.fit(\n",
    "                    [batch_inputs['user_id'], batch_inputs['video_id'], batch_inputs['video_features']],\n",
    "                    batch_labels,\n",
    "                    epochs=1,\n",
    "                    verbose=0,\n",
    "                    validation_data=(\n",
    "                        [val_inputs['user_id'], val_inputs['video_id'], val_inputs['video_features']],\n",
    "                        val_labels\n",
    "                    ) if i % 10 == 0 else None # validate every 10 batches\n",
    "                )\n",
    "        \n",
    "        val_loss, val_acc = model.evaluate(\n",
    "            [val_inputs['user_id'], val_inputs['video_id'], val_inputs['video_features']],\n",
    "            val_labels,\n",
    "            verbose=1\n",
    "        )\n",
    "        print(f\"Epoch {epoch+1} validation - Loss: {val_loss:.4f}, Mean Error: {val_acc:.4f}\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "03857adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model in batches of 50000...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 226/226 [43:06<00:00, 11.44s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   46/39159\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 1ms/step - loss: 2.0360 - mae: 0.6395   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39159/39159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1ms/step - loss: 2.8023 - mae: 0.6272\n",
      "Epoch 1 validation - Loss: 2.6797, Mean Error: 0.6266\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 226/226 [43:10<00:00, 11.46s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   46/39159\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 1ms/step - loss: 2.0814 - mae: 0.6728   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39159/39159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1ms/step - loss: 2.8323 - mae: 0.6598\n",
      "Epoch 2 validation - Loss: 2.7095, Mean Error: 0.6591\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 226/226 [43:24<00:00, 11.52s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   36/39159\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 1ms/step - loss: 1.5259 - mae: 0.6247   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39159/39159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1ms/step - loss: 2.7931 - mae: 0.6129\n",
      "Epoch 3 validation - Loss: 2.6700, Mean Error: 0.6124\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 226/226 [43:42<00:00, 11.60s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   39/39159\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 1ms/step - loss: 1.7069 - mae: 0.6542   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39159/39159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1ms/step - loss: 2.8216 - mae: 0.6455\n",
      "Epoch 4 validation - Loss: 2.6981, Mean Error: 0.6448\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 226/226 [43:50<00:00, 11.64s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   44/39159\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 1ms/step - loss: 1.9771 - mae: 0.6422   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39159/39159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1ms/step - loss: 2.8055 - mae: 0.6295\n",
      "Epoch 5 validation - Loss: 2.6822, Mean Error: 0.6289\n"
     ]
    }
   ],
   "source": [
    "model = train_model_in_batches(model, big_matrix, video_feat_dict, feature_cols)\n",
    "model.save(\"my_model3.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd95ea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(model, user_id, video_features, video_feat_dict, feature_cols, n=10, \n",
    "                            exclude_videos=None, batch_size=1000):\n",
    "    \"\"\"Generate recommendations in batches to avoid memory issues\"\"\"\n",
    "    if exclude_videos is None:\n",
    "        exclude_videos = set()\n",
    "    else:\n",
    "        exclude_videos = set(exclude_videos)\n",
    "        \n",
    "    candidate_videos = [vid for vid in video_feat_dict.keys() if vid not in exclude_videos]\n",
    "    \n",
    "    all_scores = []\n",
    "    all_video_ids = []\n",
    "    \n",
    "    for i in range(0, len(candidate_videos), batch_size):\n",
    "        batch_videos = candidate_videos[i:i+batch_size]\n",
    "        \n",
    "        user_ids = np.full(len(batch_videos), user_id, dtype=np.int32)\n",
    "        video_ids = np.array(batch_videos, dtype=np.int32)\n",
    "        features = np.array([video_feat_dict[vid] for vid in batch_videos], dtype=np.float32)\n",
    "        \n",
    "        scores = model.predict([user_ids, video_ids, features], verbose=0).flatten()\n",
    "        \n",
    "        all_scores.extend(scores)\n",
    "        all_video_ids.extend(batch_videos)\n",
    "    \n",
    "    if not all_scores:\n",
    "        return []\n",
    "        \n",
    "    paired = list(zip(all_video_ids, all_scores))\n",
    "    paired.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return [vid for vid, _ in paired[:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faf8d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendations(model, test_matrix, video_feat_dict, feature_cols, k=10):\n",
    "    \"\"\"Evaluate recommendations on a test matrix using watch ratio as the target metric\"\"\"\n",
    "    print(f\"Evaluating recommendations @{k}...\")\n",
    "    \n",
    "    user_groups = test_matrix.groupby('user_id')\n",
    "    \n",
    "    ndcg_list = []\n",
    "    hit_ratio_list = []\n",
    "    mae_list = []\n",
    "    \n",
    "    for user_id, group in tqdm(user_groups):\n",
    "        user_videos = group[['video_id', 'watch_ratio']].copy()\n",
    "        \n",
    "        if len(user_videos) < 2:\n",
    "            continue\n",
    "            \n",
    "        top_actual_videos = set(user_videos.sort_values('watch_ratio', ascending=False)['video_id'].head(k).tolist())\n",
    "        \n",
    "        all_test_videos = list(user_videos['video_id'].unique())\n",
    "        \n",
    "        user_ids = np.full(len(all_test_videos), user_id, dtype=np.int32)\n",
    "        video_ids = np.array(all_test_videos, dtype=np.int32)\n",
    "        \n",
    "        valid_videos = []\n",
    "        valid_indices = []\n",
    "        features_list = []\n",
    "        \n",
    "        for i, vid in enumerate(all_test_videos):\n",
    "            if vid in video_feat_dict:\n",
    "                valid_indices.append(i)\n",
    "                valid_videos.append(vid)\n",
    "                features_list.append(video_feat_dict[vid])\n",
    "        \n",
    "        if len(valid_videos) < 2:\n",
    "            continue\n",
    "            \n",
    "        video_to_watch_ratio = dict(zip(user_videos['video_id'], user_videos['watch_ratio']))\n",
    "            \n",
    "        user_ids = user_ids[valid_indices]\n",
    "        video_ids = np.array(valid_videos, dtype=np.int32)\n",
    "        features = np.array(features_list, dtype=np.float32)\n",
    "        \n",
    "        pred_watch_ratios = model.predict([user_ids, video_ids, features], verbose=0).flatten()\n",
    "        \n",
    "        # Calculate MAE\n",
    "        actual_ratios = np.array([video_to_watch_ratio[vid] for vid in valid_videos])\n",
    "        mae = np.mean(np.abs(actual_ratios - pred_watch_ratios))\n",
    "        mae_list.append(mae)\n",
    "        \n",
    "        video_scores = list(zip(valid_videos, pred_watch_ratios))\n",
    "        video_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top K recommendations\n",
    "        top_preds = [vid for vid, _ in video_scores[:k]]\n",
    "        \n",
    "        # Calculate hit ratio\n",
    "        hits = len(set(top_preds) & top_actual_videos)\n",
    "        hit_ratio = hits / len(top_actual_videos)\n",
    "        hit_ratio_list.append(hit_ratio)\n",
    "        \n",
    "        # Calculate NDCG\n",
    "        relevance = [1 if vid in top_actual_videos else 0 for vid, _ in video_scores[:k]]\n",
    "        \n",
    "        # Calculate DCG\n",
    "        dcg = sum([(2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(relevance)])\n",
    "        \n",
    "        # Calculate ideal DCG\n",
    "        ideal_relevance = sorted(relevance, reverse=True)\n",
    "        idcg = sum([(2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(ideal_relevance)])\n",
    "        \n",
    "        # Calculate NDCG\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "        ndcg_list.append(ndcg)\n",
    "    \n",
    "    avg_hit_ratio = np.mean(hit_ratio_list) if hit_ratio_list else 0\n",
    "    avg_ndcg = np.mean(ndcg_list) if ndcg_list else 0\n",
    "    avg_mae = np.mean(mae_list) if mae_list else 0\n",
    "    \n",
    "    print(f\"Average Hit Ratio@{k}: {avg_hit_ratio:.4f}\")\n",
    "    print(f\"Average NDCG@{k}: {avg_ndcg:.4f}\")\n",
    "    print(f\"Average MAE: {avg_mae:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        f'hit_ratio@{k}': avg_hit_ratio,\n",
    "        f'ndcg@{k}': avg_ndcg,\n",
    "        'mae': avg_mae\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b33b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating recommendations @50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1411/1411 [04:04<00:00,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Hit Ratio@50: 0.2693\n",
      "Average NDCG@50: 0.7194\n",
      "Average MAE: 0.5446\n",
      "Recommendation Performance:\n",
      "hit_ratio@50: 0.2693\n",
      "ndcg@50: 0.7194\n",
      "mae: 0.5446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load previous model\n",
    "#model = tf.keras.models.load_model(\"my_model2.keras\")\n",
    "metrics = evaluate_recommendations(model, small_matrix, video_feat_dict, feature_cols, k=50)\n",
    "    \n",
    "print(\"Recommendation Performance:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cb0b3d",
   "metadata": {},
   "source": [
    "# Analyse de l'Implémentation du Système de Recommandation\n",
    "\n",
    "## Architecture du Modèle\n",
    "\n",
    "Pour mon système de recommandation, j'ai construit une architecture de réseau neuronal optimisée pour l'efficacité mémoire tout en conservant sa puissance prédictive. Le modèle utilise une approche à double embedding avec des représentations distinctes de 32 dimensions pour les utilisateurs et les vidéos, ce qui permet de capturer les différents modèles de préférences observés lors de mon analyse de données.\n",
    "\n",
    "L'architecture suit cette structure :\n",
    "- Couche d'embedding utilisateur (7 176 utilisateurs → 32 dimensions)\n",
    "- Couche d'embedding vidéo (10 728 vidéos → 32 dimensions)\n",
    "- Branche de traitement des caractéristiques avec couches denses pour les attributs vidéo\n",
    "- Concaténation de tous les embeddings\n",
    "- Deux couches denses (32 neurones, puis 16 neurones) avec activation ReLU\n",
    "- Couche de dropout (0,2) pour éviter le surapprentissage\n",
    "- Couche de sortie linéaire pour la prédiction du ratio de visionnage\n",
    "\n",
    "J'ai spécifiquement choisi la régression plutôt que la classification après avoir constaté la nature continue de la distribution du ratio de visionnage. Le modèle prédit les valeurs réelles du ratio plutôt qu'un engagement binaire, ce qui correspond mieux à mon objectif de recommander des vidéos avec le plus fort potentiel de visionnage.\n",
    "\n",
    "## Processus d'Entraînement\n",
    "\n",
    "L'entraînement de ce modèle sur 12,5 millions d'interactions présentait des défis de mémoire, j'ai donc implémenté un traitement par lots de 50 000 interactions. Les décisions clés d'entraînement incluaient :\n",
    "\n",
    "- Utilisation de l'erreur quadratique moyenne comme fonction de perte\n",
    "- Suivi de l'erreur absolue moyenne pendant l'entraînement\n",
    "- Implémentation d'une fonction d'entraînement par lots personnalisée pour gérer les contraintes mémoire\n",
    "- Normalisation des caractéristiques avec StandardScaler pour traiter la grande variété de valeurs\n",
    "- Création d'un dictionnaire de recherche de caractéristiques efficace pour accélérer le traitement par lots\n",
    "\n",
    "Les métriques d'entraînement ont montré des valeurs finales de :\n",
    "- Perte d'entraînement (MSE) : 2,7912\n",
    "- MAE d'entraînement : 0,6064\n",
    "- Perte de validation : 2,6682\n",
    "- MAE de validation : 0,6058\n",
    "\n",
    "La similarité entre les métriques d'entraînement et de validation indique une bonne généralisation sans surapprentissage. Le MAE d'environ 0,6 signifie que les prédictions s'écartent des ratios réels de 0,6 en moyenne, ce qui est raisonnable compte tenu de la large gamme de ratios (0,05 à 135,66).\n",
    "\n",
    "## Génération de Recommandations et Évaluation\n",
    "\n",
    "J'ai adapté le processus de génération de recommandations pour exploiter le modèle de régression en classant les vidéos selon le ratio de visionnage prédit. Cela garantit que les recommandations se concentrent sur les vidéos que les utilisateurs sont susceptibles de regarder en entier plutôt que simplement interagir avec.\n",
    "\n",
    "Les métriques d'évaluation ont montré :\n",
    "- Hit Ratio@50 : 0,2693\n",
    "- NDCG@50 : 0,7194\n",
    "- MAE : 0,5446\n",
    "\n",
    "Ces résultats révèlent des tendances intéressantes. Le Hit Ratio indique que le modèle capture environ 27% des vidéos les plus regardées par les utilisateurs. Bien que cela laisse place à l'amélioration, c'est nettement supérieur à des recommandations aléatoires.\n",
    "\n",
    "Le NDCG de 0,72 est véritablement solide, suggérant que même si mon modèle n'identifie pas toutes les vidéos pertinentes, il classe avec précision celles qu'il trouve. C'est particulièrement précieux pour les systèmes de recommandation où la qualité du classement importe souvent plus que le rappel.\n",
    "\n",
    "Le MAE d'évaluation (0,5446) est légèrement meilleur que celui d'entraînement, signe positif d'une bonne généralisation.\n",
    "\n",
    "## Analyse Critique et Travaux Futurs\n",
    "\n",
    "Le modèle performe bien sur la qualité du classement (NDCG) mais pourrait s'améliorer en couverture (Hit Ratio). Cela suggère que mon ingénierie de caractéristiques capture efficacement les signaux pour le classement mais pourrait manquer des facteurs déterminant la sélection globale des vidéos.\n",
    "\n",
    "La caractéristique du taux d'achèvement s'est avérée précieuse comme prévu par mon analyse de corrélation, mais la corrélation négative du taux d'engagement avec le ratio de visionnage nécessite plus d'investigation. Cette relation contre-intuitive suggère des modèles sous-jacents que je n'ai pas entièrement saisis.\n",
    "\n",
    "Pour les améliorations futures, je devrais :\n",
    "1. Implémenter une approche d'apprentissage à classer pour optimiser directement les métriques de classement\n",
    "2. Explorer des techniques d'échantillonnage plus sophistiquées pour mieux gérer la distribution déséquilibrée des ratios\n",
    "3. Tester un modèle hybride combinant approches collaboratives et basées sur le contenu\n",
    "4. Ajouter une modélisation séquentielle pour capturer les modèles temporels du comportement utilisateur\n",
    "5. Mener des études d'ablation pour mieux comprendre les contributions des caractéristiques\n",
    "\n",
    "Dans l'ensemble, cette implémentation démontre une performance solide pour un système de recommandation basé sur le contenu, avec une force particulière dans la qualité du classement. L'approche par régression modélise correctement la nature continue du comportement de visionnage, et l'architecture gère efficacement l'échelle du jeu de données malgré les contraintes de mémoire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976237b",
   "metadata": {},
   "source": [
    "# Analyse de l'Évolution de Mon Système de Recommandation\n",
    "\n",
    "## Première Tentative : Approche Basée sur le Contenu avec Similarité Cosinus\n",
    "\n",
    "Ma première tentative de système de recommandation s'est concentrée sur une approche basée sur le contenu utilisant la similarité cosinus. J'ai consacré beaucoup d'efforts à l'ingénierie des caractéristiques, créant un ensemble étendu de métriques dérivées pour capturer la qualité des vidéos et les schémas d'engagement :\n",
    "\n",
    "- **Métriques d'engagement** : taux d'achèvement, profondeur d'engagement, ratio de validation\n",
    "- **Métriques d'interaction** : ratio de j'aime, ratio de commentaires, ratio de partages\n",
    "- **Métriques de retour négatif** : ratio de signalements, ratio de réduction de contenus similaires\n",
    "- **Scores composés** : score de qualité, score d'engagement, impact sur les abonnements, impact sur la rétention\n",
    "- **Analyse des tendances** : calcul du score de tendance basé sur les variations quotidiennes de vues\n",
    "\n",
    "Le système utilisait le multithreading pour gérer les exigences de calcul, ce qui était nécessaire car le traitement prenait plus de deux heures sans cela. J'ai également incorporé des caractéristiques catégorielles et tenté de prendre en compte les signaux de retour positifs et négatifs.\n",
    "\n",
    "Malgré cette approche élaborée, les résultats ont été décevants :\n",
    "- Précision moyenne@50 : 0,0479\n",
    "- Rappel moyen@50 : 0,0479\n",
    "- F1 moyen@50 : 0,0479\n",
    "- Similarité Jaccard moyenne : 0,0250\n",
    "\n",
    "## Ce Qui N'a Pas Fonctionné\n",
    "\n",
    "En analysant ces métriques médiocres, j'ai identifié plusieurs problèmes clés :\n",
    "\n",
    "1. **Caractéristiques sur-ingéniées** : J'ai créé de nombreuses métriques dérivées sans valider leur corrélation avec le ratio de visionnage. Mon analyse ultérieure a montré que plusieurs de ces caractéristiques avaient des corrélations faibles, voire négatives, avec la variable cible.\n",
    "\n",
    "2. **Dilution du signal** : En incorporant tant de caractéristiques dans le calcul de similarité, j'ai probablement dilué les signaux importants avec du bruit. La similarité cosinus traitait toutes les dimensions de manière égale, alors que certaines caractéristiques étaient bien plus prédictives que d'autres.\n",
    "\n",
    "3. **Recommandations statiques** : L'approche par similarité cosinus produisait des recommandations fixes basées sur la similarité de contenu sans tenir compte de la nature dynamique des préférences utilisateurs ou du contexte d'engagement.\n",
    "\n",
    "4. **Personnalisation insuffisante** : Bien que j'aie inclus des profils utilisateurs, l'approche par similarité cosinus peinait à capturer les relations complexes et non-linéaires entre utilisateurs, vidéos et schémas d'engagement.\n",
    "\n",
    "5. **Approche binaire de l'engagement** : Mon premier modèle ne modélisait pas correctement la nature continue du ratio de visionnage, ce qui s'est avéré crucial étant donné sa distribution (0,05 à 135,66).\n",
    "\n",
    "## Pourquoi l'Approche par Réseau Neuronal a Mieux Fonctionné\n",
    "\n",
    "Mon modèle final de réseau neuronal a résolu ces limitations et montré des améliorations substantielles :\n",
    "\n",
    "1. **Importance des caractéristiques basée sur les données** : Au lieu de pondérer manuellement les caractéristiques dans un score composite, le réseau neuronal a appris l'importance de chaque caractéristique directement à partir des données. Mon analyse de corrélation a montré que le taux d'achèvement était le signal positif le plus important (0,07), tandis que le modèle pouvait découvrir d'autres schémas subtils.\n",
    "\n",
    "2. **Relations non-linéaires** : Les couches denses avec activations ReLU ont capturé des relations complexes non-linéaires entre les caractéristiques et le ratio de visionnage que la similarité cosinus ne pouvait pas modéliser.\n",
    "\n",
    "3. **Personnalisation via embeddings** : Les embeddings de 32 dimensions pour les utilisateurs et les vidéos ont capturé des facteurs latents que la simple correspondance de contenu ne détectait pas. C'était particulièrement important compte tenu de la forte variance dans le comportement utilisateur (ratios de visionnage de 0,13 à 2,36).\n",
    "\n",
    "4. **Régression vs similarité** : En prédisant directement le ratio de visionnage comme valeur continue plutôt qu'en calculant des scores de similarité, le modèle s'alignait mieux avec l'objectif réel de recommandation - trouver les vidéos avec le ratio de visionnage attendu le plus élevé.\n",
    "\n",
    "5. **Meilleure approche d'évaluation** : Passer de la précision/rappel au Hit Ratio et NDCG a fourni des métriques qui reflétaient mieux la qualité du classement, ce qui est essentiel dans les systèmes de recommandation.\n",
    "\n",
    "## Leçons Apprises\n",
    "\n",
    "Cette expérience m'a enseigné plusieurs leçons précieuses sur les systèmes de recommandation :\n",
    "\n",
    "1. **Commencer par l'analyse des données** : Mon approche finale a débuté par une EDA approfondie, révélant des insights que ma première tentative avait manqués, comme l'importance du taux d'achèvement et la corrélation négative du taux d'engagement.\n",
    "\n",
    "2. **Plus simple peut être meilleur** : Malgré moins de caractéristiques ingéniées, le modèle de réseau neuronal a bien mieux performé en apprenant directement les bons schémas à partir des données.\n",
    "\n",
    "3. **Choisir la bonne approche pour votre cible** : Aligner la sortie du modèle (prédiction du ratio de visionnage) avec l'objectif de recommandation (vidéos avec le ratio le plus élevé) a été crucial pour le succès.\n",
    "\n",
    "4. **Les bonnes métriques sont importantes** : Utiliser Hit Ratio et NDCG a donné une image bien plus claire de la qualité des recommandations que les métriques standard de précision/rappel utilisées initialement.\n",
    "\n",
    "L'amélioration de 0,048 de précision/rappel à 0,269 de Hit Ratio représente plus de 5 fois la performance initiale, avec l'avantage d'une excellente qualité de classement (0,719 NDCG). Cela démontre qu'une approche neuronale ciblée et basée sur les données surpasse même l'ingénierie de caractéristiques la plus soignée quand il s'agit de capturer la dynamique complexe des interactions utilisateur-vidéo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_kernel",
   "language": "python",
   "name": "bigdata_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
